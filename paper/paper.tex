\pdfoutput=1
\documentclass[11pt]{article}
\usepackage{acl}
% Standard package includes
\usepackage{times}
\usepackage{latexsym}
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{microtype}
\usepackage{float}

\title{\textit{A young boy with a tree made of trees}: \\ Domain Adaptation of an LSTM-based Image Caption Generation Model}

\author{Dominik KÃ¼nkele \\
  Master in Language Technology \\
  University of Gothenburg \\
  \texttt{guskunkdo@student.gu.se} \\\And
  Maria Irena Szawerna \\
  Master in Language Technology \\
  University of Gothenburg \\
  \texttt{gusszawma@student.gu.se} \\}

\begin{document}
\maketitle
\begin{abstract}

Within this project we aim to explore how finetuning a CNN and LSTM-based image captioning model on a new dataset influences its performance on both the old and new data, along with what role some of the parameters, such as finetuning data size or the prevalence of out-of-vocabulary tokens in captions play in the process. We discover that differences in vocabulary play a major role and that providing too much fine-tuning data can cause the whole system to fail. We conclude that domain adaptation can be difficult to carry out, but perhaps worth attempting in certain cases.\footnote{MS} 

\end{abstract}

\section{Introduction}

\textit{Here we will have the introduction: some general talk, a little summary of the background reading (do we need more reading?), our questions that we want answered.}

Automated image caption generation is an intersectional topic, combining advances in the fields of computer vision and natural language processing. It is not only an interesting and challenging task, but also one with useful real-life implementations, such as assisting people with vision impairments in a variety of ways. It is therefore quite relevant to explore a variety of issues that can be encountered in the field, in order to improve the performance of potential models and increase their relevance in practical applications.\footnote{MS}

One potential issue with image captioning systems is that of domain adaptation, meaning adapting a trained system to somewhat different data. While it is not uncommon to encounter e.g. pre-trained image processing components in image caption generation models, it would appear that the general consensus is to train a new captioning model for every new dataset. This, however, may lead to an unnecessary waste of resources, especially if large amounts of data are used for training; another potential issue is there not always being enough data to train a model from scratch. An alternative in that case would be domain adaptation.\footnote{MS}

Some more lightweight, but well-performing image captioning architectures include ones with a CNN-based image processing component and LSTM-based language generation component; their performance can be further increased by incorporating some form of attention, which allows for a more precise identification of what is relevant to the caption generation at what point. The particular model used in this project is based on \cite{xu2015attend}, with the implementation by \cite{ilinykh}. In the model, the encoder is a convolutional network, from which representations of elements of the picture - feature vectors - are obtained. The decoder is an LSTM network with attention, determining what parts of the image are the most relevant for generating a given element of the caption (ADD WHICH KIND OF ATTENTION?).\footnote{MS} 

Another way of employing attention in a similar model is presented in \cite{lu16}, where a special "visual sentinel" helps determine when to attend to the data from the image and when to the information from the language model, as not all the words are equally represented in an image - for instance, articles or other determiners, particles, etc., stem more from the requirements of a given grammatical structure or content word rather than the scene that is being described. However, since this project did not delve into the contribution of each of these elements in caption generation, it was found to be superfluous in our particular case, even though it can lead to improved performance.\footnote{MS}

The topic of domain adaptation in caption generation itself does not appear to be widely studied. However, \cite{hesselSW15} touche upon it in their paper on the importance of the language model versus the image processing component, where they prove that a good language model can produce decent captions even with poor image input. However, what the authors actually discuss is only fine-tuning the image recognition element of the model. While that is naturally helpful, especially since models of that kind require massive amounts of training data, it does not fully address the issue of fine-tuning a whole image caption generation model, and, in fact, utilizing pre-trained CNN components has become rather common.\footnote{MS}

While all of the aforementioned papers seem to approach this issue as "generation from visual input," \cite{bernardi16} describe other approaches to the issue of image caption generation, showing that this is simply one of the possible routes. Some shortcomings of this approach that they name include the inability of the system to generate captions for visual input where the system cannot recognize any salient elements. They also tackle the issue of evaluation, listing the variety of measures that can be used to that end when it comes to caption generation (as, naturally, simpler ones like accuracy would not be a good match); they admit that human judgement or measures that mimic human judgement are key when it comes to evaluating automatically generated captions.\footnote{MS}

Given how little information we could fine on the fine-tuning of CNN+LSTM image captioning models, within this project we would like to explore the extent to which a trained model of such architecture can be adapted to another domain and how some of the hyperparameters of that process can influence the outcome. We expect this process to be effective to an extent, but it may also break the model at some point. We hope for the fine-tuned model to perform better on the images from the new domain than the non-fine-tuned one, which we aim to evaluate using human judgement. Throughout the project, Dominik focused on the influence of hyperparameters, while Maria on the evaluation, with both authors inventing ways to adapt the model to their needs and identify and fix potential issues.\footnote{MS} 

In Section 2 we describe the materials and methods used to address the aforementioned questions, such as the architecture of the model, the datasets, and the evaluation methods. In Section 3 we present the results of our inquiry. In Section 4 we discuss how they answer our questions and how they relate to previous work. Finally, in Section 5 we offer conclusions and suggestions for future projects on this topic.\footnote{MS}

\section{Materials and Methods}

\textit{Here we need to describe the databases that we used and the code that we used, and what we added on our own. Then we need to describe how we went about evaluation.}

Within this section we will describe the resources that we used in the project. We will specify the characteristics of the image captioning model, the two datasets that were used to train or fine-tune it, and the questionnaire we utilized to elicit human judgements of the best captions.\footnote{MS}

\subsection{The image captioning model \textbf{DK}}
\subsection{The code \textbf{MS and DK}}

\textit{Write about the division into files, a bit what's what, what we added and what was there. I did the evaluation part. I doubt we need to describe it in much detail, can always say that the readme has instructions and all the elements are documented, but I'd still have a bit of a summary of what we did here.}

The testing and evaluation part was conducted in a variety of Jupyter Notebook files. While the initial plan was to contain all the information in one file; unfortunately, due to GitHub limitations, that Notebook would have been too large to upload due to the number of generated ans printed image and caption pairs. Therefore, all training data sized but 2k and 5k were tested in separate notebooks using the same methods, adapted from \cite{ilinykh}, stored in a separate file, and imported into the notebook. The captions were generated using beam search and then presented together with the mappings of the attention on the image. \footnote{MS}

Other than relatively minor fixes that were required to make the code run, we decided to manually exclude the UNK token from the generation process by enforcing the lowest possible probability on it in the beam search. While this meant that we could not evaluate to what extent the models favored generating that token, it did allow us to obtain captions that sounded more natural, which was paramount for the subsequent human evaluation.\footnote{MS} 

\subsection{Flickr8k \textbf{DK}}
\subsection{imageCLEF \textbf{MS}}

The second dataset used in the project was the IAPR TC-12 Benchmark, called imageCLEF in this paper. This dataset consists of 20,000 images from a variety of real life situations. Each image is paired with a description consisting of a number of consecutive utterances specifying what the image is depicting \cite{imageclef}. The assumption made in this project is that the first fragment of the description contains the most salient elements. Unlike Flickr8k, imageCLEF contains many more images that do not include people or animals. Additionally, the way the descriptions are formulated is also different from Flickr8k.

\subsection{Human judgement questionnaire \textbf{MS}}

Having produced results (captions) using the model and the data described in the previous sections, we decided to test which of the plausible-sounding captions were the best match for the images. The only models that were somewhat consistent producing coherent captions were the non-fine-tuned (original) model and the "100" training data size models (with 4 variations of UNK filter: 0.1, 0.15, 0.2, no filter). In order to collect judgements, 20 images from 3 test sets each were collected, together with the captions generated by the aforementioned models. The test sets were: unfiltered imageCLEF, filtered (max 10\% UNK tokens) imageCLEF, and Flickr8k. The questionnaire was constructed and hosted using Google Forms.

The questionnaire itself consisted of five sections. In the first one, a modicum of personal information, such as age, gender, education, native language was collected. The following three sections were constructed the same way, and each of them corresponded to a different test set. In each of those sections, 20 images, along with the corresponding captions were presented. The order of the captions for each image was randomized to avoid bias. The participants were instructed to select one caption for each image which they thought was the best fit or the best description of the image. At the end of every section the participants had an opportunity to voice their general thoughts on the captions. Finally, in the last section the participants had the chance to include their final thoughts and opinions before submitting their answers. What is important to mention is that sometimes the model could not generate a caption for a given image, or the captions were the same across multiple models. Luckily, there were always at least two different captions. Nevertheless, this does make the interpretation of the results somewhat more difficult.

The form was subsequently distributed online, and the responses were later processed and a summary will be presented in the Results section.

\section{Results \textbf{MS and DK?}}

\textit{Here we should summarize our results, in terms of:
\begin{enumerate}
    \item where did the performance exceed the best prior performance (we got BEST checkpoints?) \textbf{DK}
    \item which models actually generated reasonable captions? where did they start falling apart? \textbf{MS or DK}
    \item subjectively, which captions were best? include the questionnaire results here or subsequently \textbf{MS}
\end{enumerate}
Include figures and tables as much as we can/as much as is reasonable.}

\subsection{Hyperparameters \textbf{DK}}

\subsection{Caption Evaluation \textbf{MS}}

As described previously in the Materials and Methods section, the only models that consistently produced captions for almost all test images were the original and the "100" models. The "200" models still generated most of the captions, but many of them started being rather nonsensical or non-grammatical. This process continued in the "500" and "1000" models with fewer and fewer captions being generated successfully, and those generated ones being absolutely nonsensical. Finally in the "2k" and "5k" models most captions could not be generated.\footnote{MS} \textit{This part can be moved somewhere else if it fits better for your hyperparameters section.}

The questionnaire was filled out by 6 people. While we would like a larger sample size, we believe that we can still draw some preliminary conclusions from their answers, and that it is sufficient taking the scope of the project into account. It is also likely that the length of the questionnaire could have put off other potential respondents. The people who filled the questionnaire out ranged between 25 and 31 years of age. Three of them identified as female, two as male, one selected "other" as a response in that option. Half of the respondents had an MA/MSc degree or equivalent, two a BA/BSc or equivalent, and one selected secondary school education as the highest completed level of education. None of the people who filled the questionnaire out were native speakers of English. More detailed data about their native language was not collected. 

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{1cm}|p{4.5cm}|}
\hline \bf Image & \bf Best model (\% responses) \\ \hline
15551 & 100\_0.1 (50\%)  \\
20144 & original (50\%)  \\
20343 & 100\_0.1 (33.3\%) or 100\_1.0 (33.3\%)  \\
12761 & original (33.3\%) or 100\_1.0 (33.3\%)  \\
22381 & 100\_0.1 (83.3\%) \\
10821 & 100\_0.1 (100\%) \\
18448 & original (100\%) \\
17173 & original (66.7\%) \\
13152 & original (66.7\%)  \\
20272 & original (100\%)  \\
11310 & 100\_0.1 (66.7\%)  \\
14183 & 100\_1.0 (50\%)  \\
10622 & original (50\%)  \\
13123 & original (100\%)   \\
19181 & 100\_1.0 (83.3\%) \\
16920 & 100\_0.1 (100\%) \\
11308 & original (50\%) \\
14016 & original (83.3\%) \\
21029 & original (100\%) \\
18355 & original (33.3\%) or 100\_0.15 (33.3\%) or 100\_0.2 (33.3\%) \\
\hline
\end{tabular}
\end{center}
\caption{\label{unfiltered-table} Best captions per image in the unfiltered imageCLEF test set. }
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{1cm}|p{4.5cm}|}
\hline \bf Image & \bf Best model (\% responses) \\ \hline
40416 & original (83.3\%)  \\
39158 & original (50\%)  \\
25053 & original (83.3\%)  \\
30620 & 100\_1.0 (66.7\%)  \\
32397 & 100\_0.2 (66.7\%) \\
38937 & original (50\%) \\
39005 & 100\_0.1 (33.3\%) or 100\_0.15 (33.3\%) or 100\_0.2 (33.3\%) \\
40120 & original (66.7\%) \\
23588 & 100\_1.0 (83.3\%)  \\
39472 & 100\_0.1 (66.7\%)  \\
30138 & original (50\%)  \\
40202 & 100\_0.1 (83.3\%)  \\
35895 & original (66.7\%)  \\
32663 & 100\_0.2 (83.3\%)   \\
30705 & original (83.3\%) \\
38081 & 100\_0.15 (66.7\%) \\
31571 & original (100\%) \\
35858 & original (66.7\%) \\
37836 & 100\_0.1 (33.3\%) or 100\_0.15 (33.3\%) \\
39239 & 100\_0.15, 100\_0.2, and 100\_1.0 (50\%)\footnote{These models generated identical captions.} \\
\hline
\end{tabular}
\end{center}
\caption{\label{filtered-table} Best captions per image in the filtered imageCLEF test set. }
\end{table}

\begin{table}[h]
\begin{center}
\begin{tabular}{|p{2cm}|p{3.5cm}|}
\hline \bf Image & \bf Best model (\% responses) \\ \hline
2116444946\_ 1f5d1fe5d1 & original (100\%)  \\
2316097768\_ ef662f444b & original (100\%)  \\
2439384468\_ 58934deab6 & original (66.7\%)  \\
2112921744\_ 92bf706805 & 100\_0.2 (50\%)  \\
2392460773\_ 2aa01eb340 & original (100\%) \\
2434006663\_ 207a284cec & original (100\%) \\
2308256827\_ 3c0a7d514d & original (100\%) \\
2111360187\_ d2505437b7 & original (66.7\%) \\
2271671533\_ 7538ccd556 & original (66.7\%)  \\
2328616978\_ fb21be2b87 & original (100\%)  \\
2456907314\_ 49bc4591c4 & original (66.7\%)  \\
2229179070\_ dc8ea8582e & 100\_0.1 (100\%)  \\
2279980395\_ 989d48ae72 & 100\_0.1 (88.3\%)  \\
2393971707\_ bce01ae754 & original (83.3\%)   \\
211277478\_ 7d43aaee09 & 100\_0.1 (50\%) \\
2337919839\_ df83827fa0 & original (83.3\%) \\
2447035752\_ 415f4bb152 & original (50\%) \\
23445819\_ 3a458716c1 & 100\_0.2 (66.7\%) \\
2448210587\_ 9fe7ea5f42 & original (33.3\%) \\
2445654384\_ 4ee3e486e1 & 100\_0.1 (50\%) or 100\_0.2 (50\%)\\
\hline
\end{tabular}
\end{center}
\caption{\label{flickr-table} Best captions per image in the Flickr8k test set. }
\end{table}

\section{Discussion \textbf{MS and/or DK}}

\textit{Here we compare our results to our initial expectations and decide how they answered questions, we also contrast them with prior work (if there is any). Highlight why what we did was relevant to the field.}

\section{Conclusions and further work \textbf{MS and/or DK}}

\textit{Here we should summarize what we have done and suggest what more could be done on this topic.}

\begin{table}
\centering
\begin{tabular}{lc}
\hline
\textbf{This is} & \textbf{a table}\\
\hline
a table & with things \\
and even & more things  \\\hline
\end{tabular}
\caption{Example table.}
\label{tab:accents}
\end{table}

% Entries for the entire Anthology, followed by custom entries
\bibliography{anthology,custom}

\appendix

\section{Example Appendix}
\label{sec:appendix}

\textit{Here we should link our repository and also make sure that it is well-organized and has proper READMEs.}

\end{document}
