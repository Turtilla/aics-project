% Use this file for citations not found in the ACL Anthology (contained in "anthology.bib").
@article{xu2015attend,
  title     = {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author    = {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and Cho, Kyunghyun and Courville, Aaron C. and Salakhutdinov, Ruslan and Zemel, Richard S. and Bengio, Yoshua},
  year      = 2015,
  journal   = {CoRR},
  volume    = {abs/1502.03044},
  url       = {http://dblp.uni-trier.de/db/journals/corr/corr1502.html#XuBKCCSZB15},
  added-at  = {2017-09-17T23:46:14.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/2e3d7fad14401da21e0885ef843872ebf/nosebrain},
  ee        = {http://arxiv.org/abs/1502.03044},
  interhash = {4358528daf0cc2c9d2d99afe51772d6f},
  intrahash = {e3d7fad14401da21e0885ef843872ebf},
  keywords  = {attention image network neural recommendation tag tagnn},
  timestamp = {2017-09-17T23:46:14.000+0200}
}
@inbook{ghanimifard19,
  title  = {Knowing When to Look for What and Where: Evaluating Generation of Spatial Descriptions with Adaptive Attention: Munich, Germany, September 8-14, 2018, Proceedings, Part IV},
  author = {Ghanimifard, Mehdi and Dobnik, Simon},
  year   = 2019,
  month  = {01},
  pages  = {153--161},
  doi    = {10.1007/978-3-030-11018-5_14},
  isbn   = {978-3-030-11017-8}
}
@article{hesselSW15,
  title      = {Image Representations and New Domains in Neural Image Captioning},
  author     = {Jack Hessel and Nicolas Savva and Michael J. Wilber},
  year       = 2015,
  journal    = {CoRR},
  volume     = {abs/1508.02091},
  url        = {http://arxiv.org/abs/1508.02091},
  eprinttype = {arXiv},
  eprint     = {1508.02091},
  timestamp  = {Mon, 13 Aug 2018 16:48:13 +0200},
  biburl     = {https://dblp.org/rec/journals/corr/HesselSW15.bib},
  bibsource  = {dblp computer science bibliography, https://dblp.org}
}
@misc{lu16,
  title     = {Knowing When to Look: Adaptive Attention via A Visual Sentinel for Image Captioning},
  author    = {Lu, Jiasen and Xiong, Caiming and Parikh, Devi and Socher, Richard},
  year      = 2016,
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.1612.01887},
  url       = {https://arxiv.org/abs/1612.01887},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computer Vision and Pattern Recognition (cs.CV), Artificial Intelligence (cs.AI), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{imageclef,
  title   = {The IAPR TC12 Benchmark: A New Evaluation Resource for Visual Information Systems},
  author  = {Grubinger, Michael and Clough, Paul and Müller, Henning and Deselaers, Thomas},
  year    = 2006,
  month   = 10,
  journal = {Workshop Ontoimage},
  pages   = {}
}
@article{flickr8k,
  title     = {Framing Image Description as a Ranking Task: Data, Models and Evaluation Metrics.},
  author    = {Hodosh, Micah and Young, Peter and Hockenmaier, Julia},
  year      = 2013,
  journal   = {J. Artif. Intell. Res.},
  volume    = 47,
  pages     = {853--899},
  url       = {http://dblp.uni-trier.de/db/journals/jair/jair47.html#HodoshYH13},
  added-at  = {2017-06-21T00:00:00.000+0200},
  biburl    = {https://www.bibsonomy.org/bibtex/268b8d329e8f6639b01c0f6b2b500a274/dblp},
  ee        = {https://doi.org/10.1613/jair.3994},
  interhash = {0f2a4f1b0f3348c19e7922674b42ff04},
  intrahash = {68b8d329e8f6639b01c0f6b2b500a274},
  keywords  = {dblp},
  timestamp = {2017-06-22T11:37:34.000+0200}
}
@misc{ilinykh,
  title   = {Image Captioning tutorial},
  author  = {Ilinykh, Nikolai},
  journal = {GitHub},
  url     = {https://github.com/sdobnik/aics/tree/master/tutorials/03-image-captioning/2022}
}
@misc{bernardi16,
  title     = {Automatic Description Generation from Images: A Survey of Models, Datasets, and Evaluation Measures},
  author    = {Bernardi, Raffaella and Cakici, Ruket and Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and Ikizler-Cinbis, Nazli and Keller, Frank and Muscat, Adrian and Plank, Barbara},
  year      = 2016,
  publisher = {arXiv},
  doi       = {10.48550/ARXIV.1601.03896},
  url       = {https://arxiv.org/abs/1601.03896},
  copyright = {arXiv.org perpetual, non-exclusive license},
  keywords  = {Computation and Language (cs.CL), Computer Vision and Pattern Recognition (cs.CV), FOS: Computer and information sciences, FOS: Computer and information sciences}
}
@article{Ilinykh2021,
  author    = {Nikolai Ilinykh and Simon Dobnik},
  journal   = {Frontiers in Artificial Intelligence},
  title     = {What Does a Language-And-Vision Transformer See: The Impact of Semantic Information on Visual Representations},
  year      = {2021},
  month     = {dec},
  volume    = {4},
  doi       = {10.3389/frai.2021.767971},
  file      = {:papers/Ilinykh2021 - What Does a Language and Vision Transformer See_ the Impact of Semantic Information on Visual Representations.pdf:PDF},
  groups    = {AI: Cognitive System, Project},
  publisher = {Frontiers Media {SA}}
}
@inproceedings{Ilinykh2021a,
  author     = {Nikolai Ilinykh; Simon Dobnik},
  title      = {How Vision Affects Language: Comparing Masked Self-Attention in Uni-Modal and Multi-Modal Transformer},
  year       = {2021},
  month      = jun,
  pages      = {45–55},
  publisher  = {Association for Computational Linguistics},
  volume     = {Proceedings of the 1st Workshop on Multimodal Semantic Representations (MMSR)},
  file       = {:papers/Ilinykh2021a - How Vision Affects Language_ Comparing Masked Self Attention in Uni Modal and Multi Modal Transformer.pdf:PDF},
  groups     = {AI: Cognitive System, Project},
  readstatus = {read}
}
@inproceedings{Ilinykh2022,
  author    = {Nikolai Ilinykh and Simon Dobnik},
  booktitle = {Findings of the Association for Computational Linguistics: {ACL} 2022},
  title     = {Attention as Grounding: Exploring Textual and Cross-Modal Attention on Entities and Relations in Language-and-Vision Transformer},
  year      = {2022},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/2022.findings-acl.320},
  file      = {:papers/Ilinykh2022 - Attention As Grounding_ Exploring Textual and Cross Modal Attention on Entities and Relations in Language and Vision Transformer.pdf:PDF},
  groups    = {AI: Cognitive System, Project}
}
@inproceedings{Ghanimifard2019a,
  author    = {Mehdi Ghanimifard and Simon Dobnik},
  booktitle = {Proceedings of the 12th International Conference on Natural Language Generation},
  title     = {What goes into a word: generating image descriptions with top-down spatial knowledge},
  year      = {2019},
  publisher = {Association for Computational Linguistics},
  doi       = {10.18653/v1/w19-8668},
  file      = {:papers/Ghanimifard2019a - What Goes into a Word_ Generating Image Descriptions with Top down Spatial Knowledge.pdf:PDF},
  groups    = {AI: Cognitive System, Project}
}
@article{Lee2022,
  author        = {Jae Hee Lee and Matthias Kerzel and Kyra Ahrens and Cornelius Weber and Stefan Wermter},
  title         = {What is Right for Me is Not Yet Right for You: A Dataset for Grounding Relative Directions via Multi-Task Learning},
  year          = {2022},
  month         = may,
  abstract      = {Understanding spatial relations is essential for intelligent agents to act and communicate in the physical world. Relative directions are spatial relations that describe the relative positions of target objects with regard to the intrinsic orientation of reference objects. Grounding relative directions is more difficult than grounding absolute directions because it not only requires a model to detect objects in the image and to identify spatial relation based on this information, but it also needs to recognize the orientation of objects and integrate this information into the reasoning process. We investigate the challenging problem of grounding relative directions with end-to-end neural networks. To this end, we provide GRiD-3D, a novel dataset that features relative directions and complements existing visual question answering (VQA) datasets, such as CLEVR, that involve only absolute directions. We also provide baselines for the dataset with two established end-to-end VQA models. Experimental evaluations show that answering questions on relative directions is feasible when questions in the dataset simulate the necessary subtasks for grounding relative directions. We discover that those subtasks are learned in an order that reflects the steps of an intuitive pipeline for processing relative directions.},
  archiveprefix = {arXiv},
  eprint        = {2205.02671},
  file          = {:papers/Lee2022 - What Is Right for Me Is Not yet Right for You_ a Dataset for Grounding Relative Directions Via Multi Task Learning.pdf:PDF},
  groups        = {Project, AI: Cognitive System},
  keywords      = {cs.CV, cs.AI, cs.LG, I.2.6},
  primaryclass  = {cs.CV}
}
@article{Ahrens2022,
  author        = {Kyra Ahrens and Matthias Kerzel and Jae Hee Lee and Cornelius Weber and Stefan Wermter},
  journal       = {IJCAI 2022 Workshop on Spatio-Temporal Reasoning and Learning},
  title         = {Knowing Earlier what Right Means to You: A Comprehensive VQA Dataset for Grounding Relative Directions via Multi-Task Learning},
  year          = {2022},
  month         = jul,
  abstract      = {Spatial reasoning poses a particular challenge for intelligent agents and is at the same time a prerequisite for their successful interaction and communication in the physical world. One such reasoning task is to describe the position of a target object with respect to the intrinsic orientation of some reference object via relative directions. In this paper, we introduce GRiD-A-3D, a novel diagnostic visual question-answering (VQA) dataset based on abstract objects. Our dataset allows for a fine-grained analysis of end-to-end VQA models' capabilities to ground relative directions. At the same time, model training requires considerably fewer computational resources compared with existing datasets, yet yields a comparable or even higher performance. Along with the new dataset, we provide a thorough evaluation based on two widely known end-to-end VQA architectures trained on GRiD-A-3D. We demonstrate that within a few epochs, the subtasks required to reason over relative directions, such as recognizing and locating objects in a scene and estimating their intrinsic orientations, are learned in the order in which relative directions are intuitively processed.},
  archiveprefix = {arXiv},
  eprint        = {2207.02624},
  file          = {:papers/Ahrens2022 - Knowing Earlier What Right Means to You_ a Comprehensive VQA Dataset for Grounding Relative Directions Via Multi Task Learning.pdf:PDF},
  groups        = {Project, AI: Cognitive System},
  keywords      = {cs.CV, cs.CL},
  primaryclass  = {cs.CV}
}
@inproceedings{Dobnik2013,
  author     = {Dobnik, Simon and Kelleher, John D.},
  booktitle  = {Proceedings of PRE-CogSci 2013 Production of referring expressions -- bridging the gap between cognitive and computational approaches to reference at CogSci},
  title      = {Towards an automatic identification of functional and geometric spatial prepositions},
  year       = {2013},
  address    = {Berlin, Germany},
  month      = {31 July},
  pages      = {1--6},
  file       = {:papers/Dobnik2013 - Towards an Automatic Identification of Functional and Geometric Spatial Prepositions.pdf:PDF},
  groups     = {Project, AI: Cognitive System},
  priority   = {prio3},
  readstatus = {read},
  url        = {https://emea01.safelinks.protection.outlook.com/?url=http%3A%2F%2Fpre2013.uvt.nl%2Fpdf%2Fdobnik-kelleher.pdf&data=05%7C01%7C%7C84d038df55584d2b11b608dadea1d8b4%7C84df9e7fe9f640afb435aaaaaaaaaaaa%7C1%7C0%7C638067083956924323%7CUnknown%7CTWFpbGZsb3d8eyJWIjoiMC4wLjAwMDAiLCJQIjoiV2luMzIiLCJBTiI6Ik1haWwiLCJXVCI6Mn0%3D%7C3000%7C%7C%7C&sdata=OwxWkerv8%2BVRu8t7sYZ5FZFsJX3Sy4Ad5LVi7h2sB3Y%3D&reserved=0}
}
