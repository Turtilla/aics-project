{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.etree.ElementTree import ParseError\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# adding imports to match the ones from test.ipynb, where we will be sourcing some of the code from\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.pyplot import figure\n",
    "import skimage.transform\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# adding imports to match the ones from train.py\n",
    "import time\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from models import Encoder, DecoderWithAttention\n",
    "#from dataloader import *\n",
    "from preproc import *\n",
    "from nltk.translate.bleu_score import corpus_bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '../data/iaprtc12/'\n",
    "annotation_dir = os.path.join(data_root_dir, 'annotations_complete_eng/')\n",
    "image_dir = os.path.join(data_root_dir, 'images/')\n",
    "\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "PADDING_TOKEN = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualization parameters\n",
    "model_path = '../data/BEST_checkpoint_flickr8k_5_10.pth.tar'  # model path updated\n",
    "word_map = '../data/wordmap_flickr8k_5_10.json'  # wordmap path updated\n",
    "beam_size = 3\n",
    "smooth = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data loading \n",
    "This part is mostly done by Dominik, with individual contributions by Maria marked in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@dataclass(slots=True, kw_only=True)\n",
    "class CLEFSample:\n",
    "    # by Dominik\n",
    "    image_id: str\n",
    "    caption: str\n",
    "    caption_length: torch.CharTensor\n",
    "    image_path: str\n",
    "    encoded_caption: torch.LongTensor = None\n",
    "    image: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class CLEFDataset(Dataset):\n",
    "    # by Dominik, individual contributions by Maria marked with in-line comments or comments under specific methods\n",
    "    def __init__(\n",
    "        self,\n",
    "        annotation_directory: str,\n",
    "        image_directory: str,\n",
    "        number_images=100,\n",
    "        word_map: dict = None,\n",
    "        min_frequency=10,\n",
    "        concat_captions: bool = False  # added by Maria to allow the optional concatenation of multiple captions into one\n",
    "    ) -> None:\n",
    "        super(CLEFDataset, self).__init__()\n",
    "        captions = self._load_captions(annotation_directory, number_images, concat_captions)\n",
    "        samples = self._load_images(image_directory, captions)\n",
    "\n",
    "        if word_map == None:\n",
    "            word_map = self._create_word_map(samples, min_frequency)\n",
    "        self.word_map = word_map\n",
    "\n",
    "        self.samples = self._encode_captions(samples)\n",
    "\n",
    "    def _load_captions(self, directory: str, number_images: int, concat_captions: bool) -> list[CLEFSample]:\n",
    "        captions: list[CLEFSample] = []\n",
    "\n",
    "        file_pattern = directory + '**/*.eng'\n",
    "        for file in glob(file_pattern, recursive=True):\n",
    "            if len(captions) == number_images:\n",
    "                break\n",
    "            try:\n",
    "                root = ElementTree.parse(file).getroot()\n",
    "                description = root.find('./DESCRIPTION').text\n",
    "                # multiple captions option by Maria\n",
    "                all_captions = [cleansed_caption\n",
    "                                for caption in description.split(';')\n",
    "                                if (cleansed_caption := caption.strip()) != '']\n",
    "                if concat_captions == True:\n",
    "                    first_caption = ' and '.join(all_captions)\n",
    "                else:\n",
    "                    first_caption = all_captions[0]\n",
    "\n",
    "                tokenized_caption = nltk.word_tokenize(first_caption)\n",
    "\n",
    "                image_path = root.find('./IMAGE').text.removeprefix('images/')\n",
    "                image_id = image_path.removesuffix('.jpg')\n",
    "\n",
    "                # selecting only the captions that include verbs or prepositions (relation words) by Maria\n",
    "                annotated_caption = nltk.pos_tag(tokenized_caption, tagset='universal')\n",
    "\n",
    "                va_counter = 0  # for seeing if there is a verb or an adposition in the description\n",
    "                for tagged_word in annotated_caption:\n",
    "                    if tagged_word[1] == 'VERB':\n",
    "                        va_counter += 1\n",
    "                    elif tagged_word[1] == 'ADP':\n",
    "                        va_counter += 1\n",
    "                    else:\n",
    "                        continue\n",
    "\n",
    "                if va_counter > 0:\n",
    "                    captions.append(CLEFSample(\n",
    "                        image_id=image_id,\n",
    "                        caption=tokenized_caption,\n",
    "                        # +2 for start and end token\n",
    "                        caption_length=torch.CharTensor([len(tokenized_caption) + 2]),\n",
    "                        image_path=image_path\n",
    "                    ))\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "            except ParseError:\n",
    "                continue\n",
    "\n",
    "        print('Captions loaded!')  # added for clarity by Maria\n",
    "\n",
    "        return captions\n",
    "\n",
    "    def _load_images(self, directory: str, captions: list[CLEFSample]) -> list[CLEFSample]:\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        samples: list[CLEFSample] = []\n",
    "        for sample in tqdm(captions, desc='Loading images...'):  # tqdm added because Maria is impatient\n",
    "            image_path = os.path.join(directory, sample.image_path)\n",
    "\n",
    "            # TODO correct conversion?\n",
    "            # error-handling added by Maria\n",
    "            try:\n",
    "                image = Image.open(image_path).resize((256, 256)).convert('RGB')\n",
    "                sample.image = transform(image)\n",
    "                samples.append(sample)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        print('Images loaded!')  # added for clarity by Maria\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _create_word_map(self, samples: list[CLEFSample], min_frequency: int) -> dict:\n",
    "        word_frequency = Counter()\n",
    "        for sample in samples:\n",
    "            word_frequency.update(sample.caption)\n",
    "\n",
    "        words = [word for word in word_frequency.keys() if word_frequency[word] >= min_frequency]\n",
    "\n",
    "        word_map = {word: index for index, word in enumerate(words, start=1)}\n",
    "        word_map[UNKNOWN_TOKEN] = len(word_map) + 1\n",
    "        word_map[START_TOKEN] = len(word_map) + 1\n",
    "        word_map[END_TOKEN] = len(word_map) + 1\n",
    "        word_map[PADDING_TOKEN] = 0\n",
    "\n",
    "        return word_map\n",
    "\n",
    "    def _encode_captions(self, samples: list[CLEFSample]) -> list[CLEFSample]:\n",
    "        encoded_samples: list[CLEFSample] = []\n",
    "        for sample in samples:\n",
    "            encoding = [self.get_encoded_token(START_TOKEN), *[self.get_encoded_token(token)\n",
    "                                                               for token in sample.caption], self.get_encoded_token(END_TOKEN)]\n",
    "            sample.encoded_caption = torch.LongTensor(encoding)  # changing to LongTensor to match the model (Maria)\n",
    "            encoded_samples.append(sample)\n",
    "        return encoded_samples\n",
    "\n",
    "    def get_encoded_token(self, token: str) -> int:\n",
    "        if token in self.word_map:\n",
    "            return self.word_map[token]\n",
    "        else:\n",
    "            return self.word_map[UNKNOWN_TOKEN]\n",
    "\n",
    "    def __getitem__(self, index: int) -> CLEFSample:\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(samples: list[CLEFSample]) -> dict:\n",
    "    # by Dominik\n",
    "    image_ids = []\n",
    "    captions = []\n",
    "    caption_lengths = []\n",
    "    encoded_captions = []\n",
    "    image_paths = []\n",
    "    images = []\n",
    "\n",
    "    for sample in samples:\n",
    "        image_ids.append(sample.image_id)\n",
    "        captions.append(sample.caption)\n",
    "        caption_lengths.append(sample.caption_length)\n",
    "        encoded_captions.append(sample.encoded_caption)\n",
    "        image_paths.append(sample.image_path)\n",
    "        images.append(sample.image)\n",
    "    \n",
    "    return {\n",
    "        'image_ids': image_ids,\n",
    "        'captions': captions,\n",
    "        'caption_lengths': caption_lengths,\n",
    "        'encoded_captions': pad_sequence(encoded_captions, batch_first=True),\n",
    "        'image_paths': image_paths,\n",
    "        'images': images\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images...: 100%|██████████| 50/50 [00:00<00:00, 52.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = CLEFDataset(annotation_dir, image_dir, number_images=50, min_frequency=1, concat_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset by Maria\n",
    "# remove the last optional argument for random splits, this way the seed is fixed so results are reproducible\n",
    "# QUESTION: does this need to be done any prettier?\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by Dominik\n",
    "dataloader = DataLoader(\n",
    "    train_set, \n",
    "    hyperparameters['batch_size'], \n",
    "    shuffle=True, \n",
    "    collate_fn=custom_collate, \n",
    "    drop_last=True  # added by Maria since we were told it is good to do so when working with LSTMs in the Machine Learning 2 course\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['21/21901.jpg', '21/21479.jpg', '21/21213.jpg', '21/21604.jpg', '21/21062.jpg', '21/21600.jpg', '21/21124.jpg', '21/21631.jpg', '21/21951.jpg', '21/21308.jpg', '21/21946.jpg', '21/21327.jpg', '21/21178.jpg', '21/21183.jpg', '21/21516.jpg', '21/21111.jpg', '21/21013.jpg', '21/21324.jpg', '21/21151.jpg', '21/21653.jpg', '21/21162.jpg', '21/21146.jpg', '21/21018.jpg', '21/21407.jpg', '21/21480.jpg', '21/21655.jpg', '21/21627.jpg', '21/21764.jpg', '21/21054.jpg', '21/21236.jpg', '21/21112.jpg', '21/21614.jpg']\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch['image_paths'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Testing the pretrained model (by Nikolai Ilinykh)\n",
    "This part is mostly done by Nikolai, adapted for our use by Maria (changes marked by comments in the code), with individual contributions from Dominik marked in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE FOLLOWING CODE IS TAKEN FROM TEST.IPYNB BY NIKOLAI ILINYKH\n",
    "\n",
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "    \n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((256, 256))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    #img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just \n",
    "    k_prev_words = torch.LongTensor([[word_map['<start>']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just \n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "\n",
    "    # Lists to store completed sequences, their alphas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit \n",
    "    while True:\n",
    "\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "        awe = gate * awe\n",
    "\n",
    "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "        scores = decoder.fc(h)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Add new words to sequences, alphas\n",
    "\n",
    "        prev_word_inds = prev_word_inds.cpu()\n",
    "        prev_word_inds = prev_word_inds.long()        \n",
    "\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        \n",
    "        \n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
    "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach )?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != word_map['<end>']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "    alphas = complete_seqs_alpha[i]\n",
    "\n",
    "    return seq, alphas\n",
    "\n",
    "\n",
    "def visualize_att(image_path, seq, alphas, rev_word_map, smooth=False):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param seq: caption\n",
    "    :param alphas: weights\n",
    "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    \n",
    "    figure(figsize=(10, 8), dpi=80)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([24 * 38, 24 * 38], Image.LANCZOS)\n",
    "\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "\n",
    "    for t in range(len(words)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        plt.subplot(int(np.ceil(len(words) / 5.)), 5, t + 1)\n",
    "\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=12, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.numpy(), [24 * 38, 24 * 38])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.6)\n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_images(model_path, word_map, img, beam_size, smooth):\n",
    "    # turning a piece of code by Nikolai from test.ipynb into a function\n",
    "    # Load model\n",
    "    checkpoint = torch.load(model_path, map_location=str(device))\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder = decoder.to(device)\n",
    "    decoder.eval()\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder = encoder.to(device)\n",
    "    encoder.eval()\n",
    "\n",
    "    # Load word map (word2ix)\n",
    "    with open(word_map, 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "    rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "    # Encode, decode with attention and beam search\n",
    "    seq, alphas = caption_image_beam_search(encoder, decoder, img, word_map, beam_size)\n",
    "    alphas = torch.FloatTensor(alphas)\n",
    "\n",
    "    # Visualize caption and attention of best sequence\n",
    "    visualize_att(img, seq, alphas, rev_word_map, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataloader for testing purposes, batch size 1 for ease of testing, no shuffle is needed, no drop last is needed\n",
    "test_dataloader = DataLoader(\n",
    "    test_set, \n",
    "    1, \n",
    "    shuffle=False, \n",
    "    collate_fn=custom_collate, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating captions and attention...:   0%|          | 0/5 [00:00<?, ?it/s]/tmp/ipykernel_4229/2160579877.py:23: UserWarning: The given NumPy array is not writable, and PyTorch does not support non-writable tensors. This means writing to this tensor will result in undefined behavior. You may want to copy the array to protect its data or make it writable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at ../torch/csrc/utils/tensor_numpy.cpp:199.)\n",
      "  img = torch.FloatTensor(img).to(device)\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "# generating captions and descriptions for every image in the test dataloader\n",
    "# in the future can regulate which ones get generated and displayed with if statements\n",
    "for batch in tqdm(test_dataloader, desc='Generating captions and attention...'):\n",
    "    img = os.path.join(image_dir, batch['image_paths'][0])\n",
    "    test_images(model_path, word_map, img, beam_size, smooth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 - Fine-tuning the model (by Nikolai Ilinykh)\n",
    "This part is mostly done by Nikolai, adapted for our use by Maria (changes marked by comments in the code), with individual contributions from Dominik marked in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is all from Nikolai\n",
    "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
    "    \"\"\"\n",
    "    Performs one epoch's training.\n",
    "    :param train_loader: DataLoader for training data\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
    "    :param decoder_optimizer: optimizer to update decoder's weights\n",
    "    :param epoch: epoch number\n",
    "    \"\"\"\n",
    "\n",
    "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
    "    encoder.train()\n",
    "\n",
    "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
    "    data_time = AverageMeter()  # data loading time\n",
    "    losses = AverageMeter()  # loss (per word decoded)\n",
    "    top5accs = AverageMeter()  # top5 accuracy\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    # Batches\n",
    "    for i, content in enumerate(train_loader):  # changing to fit our dataloader, TODO same in validation\n",
    "        imgs = torch.stack(content['images'])\n",
    "        caps = content['encoded_captions']\n",
    "        caplens = torch.stack(content['caption_lengths'])\n",
    "        data_time.update(time.time() - start)\n",
    "\n",
    "        # Move to GPU, if available\n",
    "        imgs = imgs.to(device)\n",
    "        caps = caps.to(device)\n",
    "        caplens = caplens.to(device)\n",
    "\n",
    "        # Forward prop.\n",
    "        imgs = encoder(imgs)\n",
    "        scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "        targets = caps_sorted[:, 1:]\n",
    "\n",
    "        # Remove timesteps that we didn't decode at, or are pads\n",
    "        # pack_padded_sequence is an easy trick to do this\n",
    "        # useful tutorial: https://gist.github.com/HarshTrivedi/f4e7293e941b17d19058f6fb90ab0fec\n",
    "        scores, *_ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "        targets, *_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(scores, targets)\n",
    "\n",
    "        # Add doubly stochastic attention regularization\n",
    "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "        # Back prop.\n",
    "        decoder_optimizer.zero_grad()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        # Clip gradients\n",
    "        if grad_clip is not None:\n",
    "            clip_gradient(decoder_optimizer, grad_clip)\n",
    "            if encoder_optimizer is not None:\n",
    "                clip_gradient(encoder_optimizer, grad_clip)\n",
    "\n",
    "        # Update weights\n",
    "        decoder_optimizer.step()\n",
    "        if encoder_optimizer is not None:\n",
    "            encoder_optimizer.step()\n",
    "\n",
    "        # Keep track of metrics\n",
    "        top5 = accuracy(scores, targets, 5)\n",
    "        losses.update(loss.item(), sum(decode_lengths))\n",
    "        top5accs.update(top5, sum(decode_lengths))\n",
    "        batch_time.update(time.time() - start)\n",
    "\n",
    "        start = time.time()\n",
    "\n",
    "        # Print status\n",
    "        if i % print_freq == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Data Load Time {data_time.val:.3f} ({data_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
    "                                                                          batch_time=batch_time,\n",
    "                                                                          data_time=data_time, loss=losses,\n",
    "                                                                          top5=top5accs))\n",
    "\n",
    "\n",
    "def validate(val_loader, encoder, decoder, criterion):\n",
    "    \"\"\"\n",
    "    Performs one epoch's validation.\n",
    "    :param val_loader: DataLoader for validation data.\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param criterion: loss layer\n",
    "    :return: BLEU-4 score\n",
    "    \"\"\"\n",
    "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
    "    if encoder is not None:\n",
    "        encoder.eval()\n",
    "\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top5accs = AverageMeter()\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
    "    hypotheses = list()  # hypotheses (predictions)\n",
    "\n",
    "    # explicitly disable gradient calculation to avoid CUDA memory error\n",
    "    # solves the issue #57\n",
    "    with torch.no_grad():\n",
    "        # Batches\n",
    "        for i, content in enumerate(val_loader):  # changing to fit our dataloader\n",
    "            imgs = torch.stack(content['images'])\n",
    "            caps = content['encoded_captions']\n",
    "            caplens = torch.stack(content['caption_lengths'])\n",
    "\n",
    "            # Move to device, if available\n",
    "            imgs = imgs.to(device)\n",
    "            caps = caps.to(device)\n",
    "            caplens = caplens.to(device)\n",
    "\n",
    "            # Forward prop.\n",
    "            if encoder is not None:\n",
    "                imgs = encoder(imgs)\n",
    "            scores, caps_sorted, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
    "\n",
    "            # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
    "            targets = caps_sorted[:, 1:]\n",
    "\n",
    "            # Remove timesteps that we didn't decode at, or are pads\n",
    "            # pack_padded_sequence is an easy trick to do this\n",
    "            scores_copy = scores.clone()\n",
    "            scores, *_ = pack_padded_sequence(scores, decode_lengths, batch_first=True)\n",
    "            targets, *_ = pack_padded_sequence(targets, decode_lengths, batch_first=True)\n",
    "\n",
    "            # Calculate loss\n",
    "            loss = criterion(scores, targets)\n",
    "\n",
    "            # Add doubly stochastic attention regularization\n",
    "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
    "\n",
    "            # Keep track of metrics\n",
    "            losses.update(loss.item(), sum(decode_lengths))\n",
    "            top5 = accuracy(scores, targets, 5)\n",
    "            top5accs.update(top5, sum(decode_lengths))\n",
    "            batch_time.update(time.time() - start)\n",
    "\n",
    "            start = time.time()\n",
    "\n",
    "            if i % print_freq == 0:\n",
    "                print('Validation: [{0}/{1}]\\t'\n",
    "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
    "                                                                                loss=losses, top5=top5accs))\n",
    "\n",
    "            # Store references (true captions), and hypothesis (prediction) for each image\n",
    "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
    "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
    "\n",
    "            # References\n",
    "            # TODO remove old references\n",
    "            # allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
    "            # for j in range(allcaps.shape[0]):\n",
    "            #     img_caps = allcaps[j].tolist()\n",
    "            #     img_captions = list(\n",
    "            #         map(lambda c: [w for w in c if w not in {word_map['<start>'], word_map['<pad>']}],\n",
    "            #             img_caps))  # remove <start> and pads\n",
    "            #     references.append(img_captions)\n",
    "\n",
    "            references.append([\n",
    "                [token for token in caption if token not in {word_map['<start>'], word_map['<pad>']}]\n",
    "                for caption in caps\n",
    "            ])\n",
    "\n",
    "            # Hypotheses\n",
    "            _, preds = torch.max(scores_copy, dim=2)\n",
    "            preds = preds.tolist()\n",
    "            temp_preds = list()\n",
    "            for j, p in enumerate(preds):\n",
    "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
    "            preds = temp_preds\n",
    "            hypotheses.extend(preds)\n",
    "\n",
    "            assert len(references) == len(hypotheses)\n",
    "        print(references)\n",
    "        print(hypotheses)\n",
    "        # Calculate BLEU-4 scores\n",
    "        bleu4 = corpus_bleu(references, hypotheses)\n",
    "\n",
    "        print(\n",
    "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
    "                loss=losses,\n",
    "                top5=top5accs,\n",
    "                bleu=bleu4))\n",
    "\n",
    "    return bleu4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model parameters\n",
    "emb_dim = 512  # dimension of word embeddings\n",
    "attention_dim = 512  # dimension of attention linear layers\n",
    "decoder_dim = 512  # dimension of decoder RNN\n",
    "dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
    "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
    "\n",
    "# Training parameters\n",
    "start_epoch = 0\n",
    "epochs = 120  # number of epochs to train for (if early stopping is not triggered)\n",
    "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
    "batch_size = 32\n",
    "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
    "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
    "decoder_lr = 4e-4  # learning rate for decoder\n",
    "grad_clip = 5.  # clip gradients at an absolute value of\n",
    "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
    "best_bleu4 = 0.  # BLEU-4 score right now\n",
    "print_freq = 100  # print training/validation stats every __ batches\n",
    "fine_tune_encoder = False  # fine-tune encoder?\n",
    "checkpoint = '../data/BEST_checkpoint_flickr8k_5_10.pth.tar'  # path to checkpoint, None if none\n",
    "\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Training and validation.\n",
    "    \"\"\"\n",
    "\n",
    "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, word_map\n",
    "\n",
    "    # Read word map\n",
    "    word_map_file = '../data/wordmap_flickr8k_5_10.json'\n",
    "    with open(word_map_file, 'r') as j:\n",
    "        word_map = json.load(j)\n",
    "\n",
    "#removing initializing model\n",
    "\n",
    "    checkpoint = torch.load(checkpoint, map_location=torch.device(device))\n",
    "    start_epoch = checkpoint['epoch'] + 1\n",
    "    epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
    "    best_bleu4 = checkpoint['bleu-4']\n",
    "    decoder = checkpoint['decoder']\n",
    "    decoder_optimizer = checkpoint['decoder_optimizer']\n",
    "    encoder = checkpoint['encoder']\n",
    "    encoder_optimizer = checkpoint['encoder_optimizer']\n",
    "    if fine_tune_encoder is True and encoder_optimizer is None:\n",
    "        encoder.fine_tune(fine_tune_encoder)\n",
    "        encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
    "                                            lr=encoder_lr)\n",
    "\n",
    "    # Move to GPU, if available\n",
    "    decoder = decoder.to(device)\n",
    "    encoder = encoder.to(device)\n",
    "\n",
    "    # Loss function\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "    # Custom dataloaders\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        train_set,  \n",
    "        shuffle=True, \n",
    "        collate_fn=custom_collate, \n",
    "        drop_last=True,\n",
    "        batch_size=batch_size, num_workers=workers, pin_memory=True)\n",
    "    val_loader = torch.utils.data.DataLoader(\n",
    "        train_set,  # change to val_set later!!!  \n",
    "        shuffle=True, \n",
    "        collate_fn=custom_collate, \n",
    "        drop_last=True,\n",
    "        batch_size=batch_size, num_workers=workers, pin_memory=True)\n",
    "\n",
    "    # Epochs\n",
    "    for epoch in range(start_epoch, epochs):\n",
    "\n",
    "        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
    "        if epochs_since_improvement == 20:\n",
    "            break\n",
    "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
    "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
    "            if fine_tune_encoder:\n",
    "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
    "\n",
    "        # One epoch's training\n",
    "        train(train_loader=train_loader,\n",
    "              encoder=encoder,\n",
    "              decoder=decoder,\n",
    "              criterion=criterion,\n",
    "              encoder_optimizer=encoder_optimizer,\n",
    "              decoder_optimizer=decoder_optimizer,\n",
    "              epoch=epoch)\n",
    "\n",
    "        # One epoch's validation\n",
    "        recent_bleu4 = validate(val_loader=val_loader,\n",
    "                                encoder=encoder,\n",
    "                                decoder=decoder,\n",
    "                                criterion=criterion)\n",
    "\n",
    "        # Check if there was an improvement\n",
    "        is_best = recent_bleu4 > best_bleu4\n",
    "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
    "        if not is_best:\n",
    "            epochs_since_improvement += 1\n",
    "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
    "        else:\n",
    "            epochs_since_improvement = 0\n",
    "\n",
    "        # Save checkpoint\n",
    "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
    "                        decoder_optimizer, recent_bleu4, is_best)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [6][0/1]\tBatch Time 22.366 (22.366)\tData Load Time 0.411 (0.411)\tLoss 10.0397 (10.0397)\tTop-5 Accuracy 4.226 (4.226)\n",
      "Validation: [0/1]\tBatch Time 10.648 (10.648)\tLoss 8.9103 (8.9103)\tTop-5 Accuracy 4.262 (4.262)\t\n"
     ]
    },
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'allcaps' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[27], line 96\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m     87\u001b[0m train(train_loader\u001b[38;5;241m=\u001b[39mtrain_loader,\n\u001b[1;32m     88\u001b[0m       encoder\u001b[38;5;241m=\u001b[39mencoder,\n\u001b[1;32m     89\u001b[0m       decoder\u001b[38;5;241m=\u001b[39mdecoder,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     92\u001b[0m       decoder_optimizer\u001b[38;5;241m=\u001b[39mdecoder_optimizer,\n\u001b[1;32m     93\u001b[0m       epoch\u001b[38;5;241m=\u001b[39mepoch)\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m# One epoch's validation\u001b[39;00m\n\u001b[0;32m---> 96\u001b[0m recent_bleu4 \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mval_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mencoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;66;03m# Check if there was an improvement\u001b[39;00m\n\u001b[1;32m    102\u001b[0m is_best \u001b[38;5;241m=\u001b[39m recent_bleu4 \u001b[38;5;241m>\u001b[39m best_bleu4\n",
      "Cell \u001b[0;32mIn[24], line 168\u001b[0m, in \u001b[0;36mvalidate\u001b[0;34m(val_loader, encoder, decoder, criterion)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation: [\u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    158\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBatch Time \u001b[39m\u001b[38;5;132;01m{batch_time.val:.3f}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{batch_time.avg:.3f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    159\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss \u001b[39m\u001b[38;5;132;01m{loss.val:.4f}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{loss.avg:.4f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    160\u001b[0m           \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTop-5 Accuracy \u001b[39m\u001b[38;5;132;01m{top5.val:.3f}\u001b[39;00m\u001b[38;5;124m (\u001b[39m\u001b[38;5;132;01m{top5.avg:.3f}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(i, \u001b[38;5;28mlen\u001b[39m(val_loader), batch_time\u001b[38;5;241m=\u001b[39mbatch_time,\n\u001b[1;32m    161\u001b[0m                                                                     loss\u001b[38;5;241m=\u001b[39mlosses, top5\u001b[38;5;241m=\u001b[39mtop5accs))\n\u001b[1;32m    163\u001b[0m \u001b[38;5;66;03m# Store references (true captions), and hypothesis (prediction) for each image\u001b[39;00m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;66;03m# If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\u001b[39;00m\n\u001b[1;32m    165\u001b[0m \u001b[38;5;66;03m# references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# References\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m allcaps \u001b[38;5;241m=\u001b[39m \u001b[43mallcaps\u001b[49m[sort_ind]  \u001b[38;5;66;03m# because images were sorted in the decoder\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(allcaps\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m    170\u001b[0m     img_caps \u001b[38;5;241m=\u001b[39m allcaps[j]\u001b[38;5;241m.\u001b[39mtolist()\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'allcaps' referenced before assignment"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('aics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0aa8b618b2a4c99189edcd4356029e9b6dd0644c3169178130b68c1aeecff1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
