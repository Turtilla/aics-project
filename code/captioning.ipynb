{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.etree.ElementTree import ParseError\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import nltk\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# adding imports to match the ones from test.ipynb, where we will be sourcing some of the code from\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.pyplot import figure\n",
    "import skimage.transform\n",
    "import argparse\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '../data/iaprtc12/'\n",
    "annotation_dir = os.path.join(data_root_dir, 'annotations_complete_eng/')\n",
    "image_dir = os.path.join(data_root_dir, 'images/')\n",
    "\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "PADDING_TOKEN = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data loading \n",
    "This part is mostly done by Dominik, with individual contributions by Maria marked in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@dataclass(slots=True, kw_only=True)\n",
    "class CLEFSample:\n",
    "    # by Dominik\n",
    "    image_id: str\n",
    "    caption: str\n",
    "    caption_length: torch.CharTensor\n",
    "    image_path: str\n",
    "    encoded_caption: torch.IntTensor = None\n",
    "    image: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class CLEFDataset(Dataset):\n",
    "    # by Dominik, individual contributions by Maria marked with in-line comments or comments under specific methods\n",
    "    def __init__(\n",
    "        self, \n",
    "        annotation_directory: str, \n",
    "        image_directory: str, \n",
    "        number_images=100, \n",
    "        word_map: dict = None, \n",
    "        min_frequency=10, \n",
    "        concat_captions: bool = False  # added by Maria to allow the optional concatenation of multiple captions into one\n",
    "    ) -> None:\n",
    "        super(CLEFDataset, self).__init__()\n",
    "        captions = self._load_captions(annotation_directory, number_images, concat_captions)\n",
    "        samples = self._load_images(image_directory, captions)\n",
    "\n",
    "        if word_map == None:\n",
    "            word_map = self._create_word_map(samples, min_frequency)\n",
    "        self.word_map = word_map\n",
    "\n",
    "        self.samples = self._encode_captions(samples)\n",
    "\n",
    "    def _load_captions(self, directory: str, number_images: int, concat_captions: bool) -> list[CLEFSample]:\n",
    "        captions: list[CLEFSample] = []\n",
    "\n",
    "        file_pattern = directory + '**/*.eng'\n",
    "        for file in glob(file_pattern, recursive=True):\n",
    "            if len(captions) == number_images:\n",
    "                break\n",
    "            try:\n",
    "                root = ElementTree.parse(file).getroot()\n",
    "                description = root.find('./DESCRIPTION').text\n",
    "                # multiple captions option by Maria\n",
    "                all_captions = description.split(';')\n",
    "                if concat_captions == True:\n",
    "                    first_caption = ' and '.join(all_captions[:-1])  # if not -1, then there is a trailing 'and'\n",
    "                else:\n",
    "                    first_caption = all_captions[0]\n",
    "                    \n",
    "                tokenized_caption = nltk.word_tokenize(first_caption)\n",
    "                \n",
    "                image_path = root.find('./IMAGE').text.removeprefix('images/')\n",
    "                image_id = image_path.removesuffix('.jpg')\n",
    "                \n",
    "                # selecting only the captions that include verbs or prepositions (relation words) by Maria\n",
    "                annotated_caption = nltk.pos_tag(tokenized_caption, tagset='universal')\n",
    "\n",
    "                va_counter = 0  # for seeing if there is a verb or an adposition in the description\n",
    "                for tagged_word in annotated_caption:\n",
    "                    if tagged_word[1] == 'VERB':\n",
    "                        va_counter += 1\n",
    "                    elif tagged_word[1] == 'ADP':\n",
    "                        va_counter += 1\n",
    "                    else:\n",
    "                        continue\n",
    "                \n",
    "                if va_counter > 0:\n",
    "                    captions.append(CLEFSample(\n",
    "                        image_id=image_id,\n",
    "                        caption=tokenized_caption,\n",
    "                        # +2 for start and end token\n",
    "                        caption_length=torch.CharTensor([len(tokenized_caption) + 2]),\n",
    "                        image_path=image_path\n",
    "                    ))\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            except ParseError:\n",
    "                continue\n",
    "        \n",
    "        print('Captions loaded!')  # added for clarity by Maria\n",
    "\n",
    "        return captions\n",
    "\n",
    "    def _load_images(self, directory: str, captions: list[CLEFSample]) -> list[CLEFSample]:\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        samples: list[CLEFSample] = []\n",
    "        for sample in tqdm(captions, desc='Loading images...'):  # tqdm added because Maria is impatient\n",
    "            image_path = os.path.join(directory, sample.image_path)\n",
    "\n",
    "            # TODO correct conversion?\n",
    "            # error-handling added by Maria\n",
    "            try:\n",
    "                image = Image.open(image_path).resize((256, 256)).convert('RGB')\n",
    "                sample.image = transform(image)\n",
    "                samples.append(sample)\n",
    "            except FileNotFoundError:\n",
    "                continue\n",
    "\n",
    "        print('Images loaded!')  # added for clarity by Maria\n",
    "        \n",
    "        return samples\n",
    "\n",
    "    def _create_word_map(self, samples: list[CLEFSample], min_frequency: int) -> dict:\n",
    "        word_frequency = Counter()\n",
    "        for sample in samples:\n",
    "            word_frequency.update(sample.caption)\n",
    "\n",
    "        words = [word for word in word_frequency.keys() if word_frequency[word] >= min_frequency]\n",
    "\n",
    "        word_map = {word: index for index, word in enumerate(words, start=1)}\n",
    "        word_map[UNKNOWN_TOKEN] = len(word_map) + 1\n",
    "        word_map[START_TOKEN] = len(word_map) + 1\n",
    "        word_map[END_TOKEN] = len(word_map) + 1\n",
    "        word_map[PADDING_TOKEN] = 0\n",
    "\n",
    "        return word_map\n",
    "\n",
    "    def _encode_captions(self, samples: list[CLEFSample]) -> list[CLEFSample]:\n",
    "        encoded_samples: list[CLEFSample] = []\n",
    "        for sample in samples:\n",
    "            encoding = [self.get_encoded_token(START_TOKEN), *[self.get_encoded_token(token)\n",
    "                                                               for token in sample.caption], self.get_encoded_token(END_TOKEN)]\n",
    "            sample.encoded_caption = torch.IntTensor(encoding)\n",
    "            encoded_samples.append(sample)\n",
    "        return encoded_samples\n",
    "\n",
    "    def get_encoded_token(self, token: str) -> int:\n",
    "        if token in self.word_map:\n",
    "            return self.word_map[token]\n",
    "        else:\n",
    "            return self.word_map[UNKNOWN_TOKEN]\n",
    "\n",
    "    def __getitem__(self, index: int) -> CLEFSample:\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(samples: list[CLEFSample]) -> dict:\n",
    "    # by Dominik\n",
    "    image_ids = []\n",
    "    captions = []\n",
    "    caption_lengths = []\n",
    "    encoded_captions = []\n",
    "    image_paths = []\n",
    "    images = []\n",
    "\n",
    "    for sample in samples:\n",
    "        image_ids.append(sample.image_id)\n",
    "        captions.append(sample.caption)\n",
    "        caption_lengths.append(sample.caption_length)\n",
    "        encoded_captions.append(sample.encoded_caption)\n",
    "        image_paths.append(sample.image_path)\n",
    "        images.append(sample.image)\n",
    "    \n",
    "    return {\n",
    "        'image_ids': image_ids,\n",
    "        'captions': captions,\n",
    "        'caption_lengths': caption_lengths,\n",
    "        'encoded_captions': pad_sequence(encoded_captions, batch_first=True),\n",
    "        'image_paths': image_paths,\n",
    "        'images': images\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Captions loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading images...: 100%|███████████████████████████████████████████████████████████████| 50/50 [00:00<00:00, 170.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = CLEFDataset(annotation_dir, image_dir, number_images=50, min_frequency=1, concat_captions=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting the dataset by Maria\n",
    "# remove the last optional argument for random splits, this way the seed is fixed so results are reproducible\n",
    "# QUESTION: does this need to be done any prettier?\n",
    "train_set, val_set, test_set = torch.utils.data.random_split(dataset, [0.8, 0.1, 0.1], generator=torch.Generator().manual_seed(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# by Dominik\n",
    "dataloader = DataLoader(\n",
    "    train_set, \n",
    "    hyperparameters['batch_size'], \n",
    "    shuffle=True, \n",
    "    collate_fn=custom_collate, \n",
    "    drop_last=True  # added by Maria since we were told it is good to do so when working with LSTMs in the Machine Learning 2 course\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for batch in dataloader:\n",
    "    #print(batch['captions'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 - Testing the pretrained model (by Nikolai Ilinykh)\n",
    "This part is mostly done by Nikolai, adapted for our use by Maria, with individual contributi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "### THE FOLLOWING CODE IS TAKEN FROM TEST.IPYNB BY NIKOLAI ILINYKH\n",
    "\n",
    "def caption_image_beam_search(encoder, decoder, image_path, word_map, beam_size=3):\n",
    "    \"\"\"\n",
    "    Reads an image and captions it with beam search.\n",
    "\n",
    "    :param encoder: encoder model\n",
    "    :param decoder: decoder model\n",
    "    :param image_path: path to image\n",
    "    :param word_map: word map\n",
    "    :param beam_size: number of sequences to consider at each decode-step\n",
    "    :return: caption, weights for visualization\n",
    "    \"\"\"\n",
    "\n",
    "    k = beam_size\n",
    "    vocab_size = len(word_map)\n",
    "    \n",
    "    \n",
    "    img = Image.open(image_path)\n",
    "    img = img.resize((256, 256))\n",
    "    img = np.transpose(img, (2, 0, 1))\n",
    "    #img = img / 255.\n",
    "    img = torch.FloatTensor(img).to(device)\n",
    "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    transform = transforms.Compose([normalize])\n",
    "    image = transform(img)  # (3, 256, 256)\n",
    "\n",
    "    # Encode\n",
    "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
    "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
    "    enc_image_size = encoder_out.size(1)\n",
    "    encoder_dim = encoder_out.size(3)\n",
    "\n",
    "    # Flatten encoding\n",
    "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
    "    num_pixels = encoder_out.size(1)\n",
    "\n",
    "    # We'll treat the problem as having a batch size of k\n",
    "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
    "\n",
    "    # Tensor to store top k previous words at each step; now they're just \n",
    "    k_prev_words = torch.LongTensor([[word_map['']]] * k).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences; now they're just \n",
    "    seqs = k_prev_words  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' scores; now they're just 0\n",
    "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
    "\n",
    "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
    "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
    "\n",
    "    # Lists to store completed sequences, their alphas and scores\n",
    "    complete_seqs = list()\n",
    "    complete_seqs_alpha = list()\n",
    "    complete_seqs_scores = list()\n",
    "\n",
    "    # Start decoding\n",
    "    step = 1\n",
    "    h, c = decoder.init_hidden_state(encoder_out)\n",
    "\n",
    "    # s is a number less than or equal to k, because sequences are removed from this process once they hit \n",
    "    while True:\n",
    "\n",
    "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
    "\n",
    "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
    "\n",
    "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
    "\n",
    "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
    "        awe = gate * awe\n",
    "\n",
    "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
    "\n",
    "        scores = decoder.fc(h)  # (s, vocab_size)\n",
    "        scores = F.log_softmax(scores, dim=1)\n",
    "\n",
    "        # Add\n",
    "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
    "\n",
    "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
    "        if step == 1:\n",
    "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
    "        else:\n",
    "            # Unroll and find top scores, and their unrolled indices\n",
    "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
    "\n",
    "        # Convert unrolled indices to actual indices of scores\n",
    "        prev_word_inds = top_k_words / vocab_size  # (s)\n",
    "        next_word_inds = top_k_words % vocab_size  # (s)\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Add new words to sequences, alphas\n",
    "\n",
    "        prev_word_inds = prev_word_inds.cpu()\n",
    "        prev_word_inds = prev_word_inds.long()        \n",
    "\n",
    "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
    "        \n",
    "        \n",
    "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
    "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
    "\n",
    "        # Which sequences are incomplete (didn't reach )?\n",
    "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
    "                           next_word != word_map[' ']]\n",
    "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
    "\n",
    "        # Set aside complete sequences\n",
    "        if len(complete_inds) > 0:\n",
    "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
    "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
    "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
    "        k -= len(complete_inds)  # reduce beam length accordingly\n",
    "\n",
    "        # Proceed with incomplete sequences\n",
    "        if k == 0:\n",
    "            break\n",
    "        seqs = seqs[incomplete_inds]\n",
    "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
    "        h = h[prev_word_inds[incomplete_inds]]\n",
    "        c = c[prev_word_inds[incomplete_inds]]\n",
    "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
    "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
    "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
    "\n",
    "        # Break if things have been going on too long\n",
    "        if step > 50:\n",
    "            break\n",
    "        step += 1\n",
    "\n",
    "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
    "    seq = complete_seqs[i]\n",
    "    alphas = complete_seqs_alpha[i]\n",
    "\n",
    "    return seq, alphas\n",
    "\n",
    "\n",
    "def visualize_att(image_path, seq, alphas, rev_word_map, smooth=False):\n",
    "    \"\"\"\n",
    "    Visualizes caption with weights at every word.\n",
    "\n",
    "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
    "\n",
    "    :param image_path: path to image that has been captioned\n",
    "    :param seq: caption\n",
    "    :param alphas: weights\n",
    "    :param rev_word_map: reverse word mapping, i.e. ix2word\n",
    "    :param smooth: smooth weights?\n",
    "    \"\"\"\n",
    "    \n",
    "    figure(figsize=(10, 8), dpi=80)\n",
    "    \n",
    "    image = Image.open(image_path)\n",
    "    image = image.resize([24 * 38, 24 * 38], Image.LANCZOS)\n",
    "\n",
    "    words = [rev_word_map[ind] for ind in seq]\n",
    "\n",
    "    for t in range(len(words)):\n",
    "        if t > 50:\n",
    "            break\n",
    "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
    "\n",
    "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
    "        plt.imshow(image)\n",
    "        current_alpha = alphas[t, :]\n",
    "        if smooth:\n",
    "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=12, sigma=8)\n",
    "        else:\n",
    "            alpha = skimage.transform.resize(current_alpha.numpy(), [24 * 38, 24 * 38])\n",
    "        if t == 0:\n",
    "            plt.imshow(alpha, alpha=0)\n",
    "        else:\n",
    "            plt.imshow(alpha, alpha=0.6)\n",
    "        plt.set_cmap(cm.Greys_r)\n",
    "        plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "' '",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m rev_word_map \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m word_map\u001b[38;5;241m.\u001b[39mitems()}  \u001b[38;5;66;03m# ix2word\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m# Encode, decode with attention and beam search\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m seq, alphas \u001b[38;5;241m=\u001b[39m \u001b[43mcaption_image_beam_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mword_map\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeam_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m alphas \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mFloatTensor(alphas)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;66;03m# Visualize caption and attention of best sequence\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 43\u001b[0m, in \u001b[0;36mcaption_image_beam_search\u001b[0;34m(encoder, decoder, image_path, word_map, beam_size)\u001b[0m\n\u001b[1;32m     40\u001b[0m encoder_out \u001b[38;5;241m=\u001b[39m encoder_out\u001b[38;5;241m.\u001b[39mexpand(k, num_pixels, encoder_dim)  \u001b[38;5;66;03m# (k, num_pixels, encoder_dim)\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# Tensor to store top k previous words at each step; now they're just \u001b[39;00m\n\u001b[0;32m---> 43\u001b[0m k_prev_words \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mLongTensor([[\u001b[43mword_map\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m]] \u001b[38;5;241m*\u001b[39m k)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# (k, 1)\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# Tensor to store top k sequences; now they're just \u001b[39;00m\n\u001b[1;32m     46\u001b[0m seqs \u001b[38;5;241m=\u001b[39m k_prev_words  \u001b[38;5;66;03m# (k, 1)\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: ' '"
     ]
    }
   ],
   "source": [
    "### THIS IS ALSO FROM NIKOLAI, ANY CHANGES THAT ARE INTRODUCED WILL BE MARKED BY COMMENTS\n",
    "\n",
    "model_path = '../data/BEST_checkpoint_flickr8k_5_10.pth.tar'  # model path updated\n",
    "img = '../data/iaprtc12/images/00/25.jpg'  # temporary image path\n",
    "\n",
    "word_map = '../data/wordmap_flickr8k_5_10.json'  # wordmap path updated\n",
    "beam_size = 3\n",
    "smooth = False\n",
    "\n",
    "# Load model\n",
    "checkpoint = torch.load(model_path, map_location=str(device))\n",
    "decoder = checkpoint['decoder']\n",
    "decoder = decoder.to(device)\n",
    "decoder.eval()\n",
    "encoder = checkpoint['encoder']\n",
    "encoder = encoder.to(device)\n",
    "encoder.eval()\n",
    "\n",
    "# Load word map (word2ix)\n",
    "with open(word_map, 'r') as j:\n",
    "    word_map = json.load(j)\n",
    "rev_word_map = {v: k for k, v in word_map.items()}  # ix2word\n",
    "\n",
    "# Encode, decode with attention and beam search\n",
    "seq, alphas = caption_image_beam_search(encoder, decoder, img, word_map, beam_size)\n",
    "alphas = torch.FloatTensor(alphas)\n",
    "\n",
    "# Visualize caption and attention of best sequence\n",
    "visualize_att(img, seq, alphas, rev_word_map, smooth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aics-project",
   "language": "python",
   "name": "aics-project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "e0aa8b618b2a4c99189edcd4356029e9b6dd0644c3169178130b68c1aeecff1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
