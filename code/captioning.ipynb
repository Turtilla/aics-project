{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0 - Imports/Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dataclasses import dataclass\n",
    "from collections import Counter\n",
    "\n",
    "from xml.etree import ElementTree\n",
    "from xml.etree.ElementTree import ParseError\n",
    "from glob import glob\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root_dir = '../data/iaprtc12/'\n",
    "annotation_dir = os.path.join(data_root_dir, 'annotations_complete_eng/')\n",
    "image_dir = os.path.join(data_root_dir, 'images/')\n",
    "\n",
    "UNKNOWN_TOKEN = '<unk>'\n",
    "START_TOKEN = '<start>'\n",
    "END_TOKEN = '<end>'\n",
    "PADDING_TOKEN = '<pad>'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameters = {\n",
    "    'batch_size': 32\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 - Data loading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass(slots=True, kw_only=True)\n",
    "class CLEFSample:\n",
    "    image_id: str\n",
    "    caption: str\n",
    "    caption_length: torch.Tensor\n",
    "    image_path: str\n",
    "    encoded_caption: torch.Tensor = None\n",
    "    image: torch.FloatTensor = None\n",
    "\n",
    "\n",
    "class CLEFDataset(Dataset):\n",
    "    def __init__(self, annotation_directory: str, image_directory: str, number_images=100, word_map: dict = None, min_frequency=10) -> None:\n",
    "        super(CLEFDataset, self).__init__()\n",
    "        captions = self._load_captions(annotation_directory, number_images)\n",
    "        samples = self._load_images(image_directory, captions)\n",
    "\n",
    "        if word_map == None:\n",
    "            word_map = self._create_word_map(samples, min_frequency)\n",
    "        self.word_map = word_map\n",
    "\n",
    "        self.samples = self._encode_captions(samples)\n",
    "\n",
    "    def _load_captions(self, directory: str, number_images: int) -> list[CLEFSample]:\n",
    "        captions: list[CLEFSample] = []\n",
    "\n",
    "        file_pattern = directory + '**/*.eng'\n",
    "        for file in glob(file_pattern, recursive=True):\n",
    "            if len(captions) == number_images:\n",
    "                break\n",
    "            try:\n",
    "                root = ElementTree.parse(file).getroot()\n",
    "                description = root.find('./DESCRIPTION').text\n",
    "                # TODO multiple captions\n",
    "                first_caption = description.split(';')[0]\n",
    "                tokenized_caption = nltk.word_tokenize(first_caption)\n",
    "\n",
    "                image_path = root.find('./IMAGE').text.removeprefix('images/')\n",
    "                image_id = image_path.removesuffix('.jpg')\n",
    "\n",
    "                captions.append(CLEFSample(\n",
    "                    image_id=image_id,\n",
    "                    caption=tokenized_caption,\n",
    "                    # +2 for start and end token\n",
    "                    caption_length=torch.Tensor([len(tokenized_caption) + 2]),\n",
    "                    image_path=image_path\n",
    "                ))\n",
    "            except ParseError:\n",
    "                continue\n",
    "\n",
    "        return captions\n",
    "\n",
    "    def _load_images(self, directory: str, captions: list[CLEFSample]) -> list[CLEFSample]:\n",
    "        transform = transforms.ToTensor()\n",
    "\n",
    "        samples: list[CLEFSample] = []\n",
    "        for sample in captions:\n",
    "            image_path = os.path.join(directory, sample.image_path)\n",
    "\n",
    "            # TODO correct conversion?\n",
    "            image = Image.open(image_path).resize((256, 256)).convert('RGB')\n",
    "            sample.image = transform(image)\n",
    "            samples.append(sample)\n",
    "\n",
    "        return samples\n",
    "\n",
    "    def _create_word_map(self, samples: list[CLEFSample], min_frequency: int) -> dict:\n",
    "        word_frequency = Counter()\n",
    "        for sample in samples:\n",
    "            word_frequency.update(sample.caption)\n",
    "\n",
    "        words = [word for word in word_frequency.keys() if word_frequency[word] >= min_frequency]\n",
    "\n",
    "        word_map = {word: index for index, word in enumerate(words, start=1)}\n",
    "        word_map[UNKNOWN_TOKEN] = len(word_map) + 1\n",
    "        word_map[START_TOKEN] = len(word_map) + 1\n",
    "        word_map[END_TOKEN] = len(word_map) + 1\n",
    "        word_map[PADDING_TOKEN] = 0\n",
    "\n",
    "        return word_map\n",
    "\n",
    "    def _encode_captions(self, samples: list[CLEFSample]):\n",
    "        encoded_samples: list[CLEFSample] = []\n",
    "        for sample in samples:\n",
    "            encoding = [self.get_encoded_token(START_TOKEN), *[self.get_encoded_token(token)\n",
    "                                                               for token in sample.caption], self.get_encoded_token(END_TOKEN)]\n",
    "            sample.encoded_caption = torch.tensor(encoding)\n",
    "            encoded_samples.append(sample)\n",
    "        return encoded_samples\n",
    "\n",
    "    def get_encoded_token(self, token: str) -> int:\n",
    "        if token in self.word_map:\n",
    "            return self.word_map[token]\n",
    "        else:\n",
    "            return self.word_map[UNKNOWN_TOKEN]\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.samples[index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_collate(samples: list[CLEFSample]) -> dict:\n",
    "    image_ids = []\n",
    "    captions = []\n",
    "    caption_lengths = []\n",
    "    encoded_captions = []\n",
    "    image_paths = []\n",
    "    images = []\n",
    "\n",
    "    for sample in samples:\n",
    "        image_ids.append(sample.image_id)\n",
    "        captions.append(sample.caption)\n",
    "        caption_lengths.append(sample.caption_length)\n",
    "        encoded_captions.append(sample.encoded_caption)\n",
    "        image_paths.append(sample.image_path)\n",
    "        images.append(sample.image)\n",
    "    \n",
    "    return {\n",
    "        'image_ids': image_ids,\n",
    "        'captions': captions,\n",
    "        'caption_lengths': caption_lengths,\n",
    "        'encoded_captions': pad_sequence(encoded_captions, batch_first=True),\n",
    "        'image_paths': image_paths,\n",
    "        'images': images\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CLEFDataset(annotation_dir, image_dir, min_frequency=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader = DataLoader(dataset, hyperparameters['batch_size'], shuffle=True, collate_fn=custom_collate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch['encoded_captions'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('aics')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e0aa8b618b2a4c99189edcd4356029e9b6dd0644c3169178130b68c1aeecff1e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
